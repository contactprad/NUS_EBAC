{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Oct 14 14:48:40 2017\n",
    "\n",
    "@author: rahul\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Find out the most similar words in the data.\n",
    "This technique can help to analyze closest relationship among words, thus help identify the\n",
    "type of accident assocated with word. Try out with different list of words and check the result. \"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Make possible for python notebooks to import the util as module. This needs to be copied at every python notebook which wants to \n",
    "load the module'''\n",
    "\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "    \n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "\n",
    "\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "    \n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "        \n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "                                       \n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "\n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "        \n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "        \n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "\n",
    "\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "    \n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "        \n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "        \n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "    \n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from tm_assignment_util.ipynb\n",
      "['catch', 'machine', 'accident', 'inspect', 'maintain', 'excavator', 'magnet', 'machine', 'maintenance', 'carry', 'make', 'jump', 'grappler', 'turn', 'excavator', 'engine', 'grappler', 'spin', 'pin', 'grappler', 'excavator']\n"
     ]
    }
   ],
   "source": [
    "# Read Files_osha\n",
    "import tm_assignment_util as util\n",
    "myutilObj = util.util()\n",
    "Osha_AccidentCases = util.accidentCases_Osha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cause</th>\n",
       "      <th>Title_Summary_Case</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exposure to extreme temperatures</td>\n",
       "      <td>Employee Is Burned By Forklift Radiator Fluid...</td>\n",
       "      <td>Employee Is Burned By Forklift Radiator Fluid</td>\n",
       "      <td>At approximately 11:30 a.m. on November 13  2...</td>\n",
       "      <td>burn  industrial truck  waste proc fac  pa...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Falls</td>\n",
       "      <td>Employee Falls From Flatbed Trailer And Later...</td>\n",
       "      <td>Employee Falls From Flatbed Trailer And Later...</td>\n",
       "      <td>On August 30  2013  Employee #1 was working f...</td>\n",
       "      <td>truck  flatbed truck  trailer  fall  abdomen</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caught in/between Objects</td>\n",
       "      <td>Employee Is Struck By Bales Of Wire And Kille...</td>\n",
       "      <td>Employee Is Struck By Bales Of Wire And Killed</td>\n",
       "      <td>On August 26  2013  Employee #1  with Lee Iro...</td>\n",
       "      <td>waste proc fac  industrial truck  struck b...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exposure to extreme temperatures</td>\n",
       "      <td>Employee Is Splashed With Hot Water And Is Bu...</td>\n",
       "      <td>Employee Is Splashed With Hot Water And Is Bu...</td>\n",
       "      <td>On July 14  2013  Employee #1  vacuum pump tr...</td>\n",
       "      <td>truck driver  pump  tank  hot water  struc...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Exposure to extreme temperatures</td>\n",
       "      <td>Employee Suffers Burns While Moving Soup .  O...</td>\n",
       "      <td>Employee Suffers Burns While Moving Soup</td>\n",
       "      <td>On June 30  2013  Employee #1 was working in ...</td>\n",
       "      <td>burn  spill  arm  chest  abdomen</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Cause  \\\n",
       "0  Exposure to extreme temperatures   \n",
       "1                             Falls   \n",
       "2         Caught in/between Objects   \n",
       "3  Exposure to extreme temperatures   \n",
       "4  Exposure to extreme temperatures   \n",
       "\n",
       "                                  Title_Summary_Case  \\\n",
       "0   Employee Is Burned By Forklift Radiator Fluid...   \n",
       "1   Employee Falls From Flatbed Trailer And Later...   \n",
       "2   Employee Is Struck By Bales Of Wire And Kille...   \n",
       "3   Employee Is Splashed With Hot Water And Is Bu...   \n",
       "4   Employee Suffers Burns While Moving Soup .  O...   \n",
       "\n",
       "                                               Title  \\\n",
       "0     Employee Is Burned By Forklift Radiator Fluid    \n",
       "1   Employee Falls From Flatbed Trailer And Later...   \n",
       "2    Employee Is Struck By Bales Of Wire And Killed    \n",
       "3   Employee Is Splashed With Hot Water And Is Bu...   \n",
       "4          Employee Suffers Burns While Moving Soup    \n",
       "\n",
       "                                             Summary  \\\n",
       "0   At approximately 11:30 a.m. on November 13  2...   \n",
       "1   On August 30  2013  Employee #1 was working f...   \n",
       "2   On August 26  2013  Employee #1  with Lee Iro...   \n",
       "3   On July 14  2013  Employee #1  vacuum pump tr...   \n",
       "4   On June 30  2013  Employee #1 was working in ...   \n",
       "\n",
       "                                          Unnamed: 4 Unnamed: 5  \n",
       "0      burn  industrial truck  waste proc fac  pa...             \n",
       "1      truck  flatbed truck  trailer  fall  abdomen              \n",
       "2      waste proc fac  industrial truck  struck b...             \n",
       "3      truck driver  pump  tank  hot water  struc...             \n",
       "4                  burn  spill  arm  chest  abdomen              "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Osha_AccidentCases['Title_Summary_Case'] = Osha_AccidentCases['Title_Summary_Case']\n",
    "#.apply(myutilObj.my_tokenizer)\n",
    "Osha_AccidentCases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Employee Is Burned By Forklift Radiator Fluid...\n",
      "1         Employee Falls From Flatbed Trailer And Later...\n",
      "2         Employee Is Struck By Bales Of Wire And Kille...\n",
      "3         Employee Is Splashed With Hot Water And Is Bu...\n",
      "4         Employee Suffers Burns While Moving Soup .  O...\n",
      "5         Employee Injures Self With Knife .  An incide...\n",
      "6         Foreman Is Fatally Crushed When Forklift Tips...\n",
      "7         Employee Fractures Abdomen When Run Over By T...\n",
      "8         Carpenter Injured In Abdomen When Saw Kicks B...\n",
      "9         Employee Abdomen And Head Injury In Slip And ...\n",
      "10        Employee Sustains Ruptured Spleen After Board...\n",
      "11        Logger Is Injured When Butt Of Tree Kicks Out...\n",
      "12        Worker Is Injured When Struck By Board And Th...\n",
      "13        Employee Suffers Burns When He Came In Contac...\n",
      "14        Bulldozer Operator Is Crushed While Working O...\n",
      "15        Employee'S Abdomen Is Penetrated By A Wood Pa...\n",
      "16        Employee Fractures Ankle Unclogging Beet Harv...\n",
      "17        Employee Is Struck By Cable  Sustains Facial ...\n",
      "18        Driver Hurt When Truck Unloading Sand Overtur...\n",
      "19        Employee Sprains Knee When Struck By Forklift...\n",
      "20        Employee Falls Through Greenhouse Roof  Fract...\n",
      "21        Employee Is Killed In Avalance .  On April 11...\n",
      "22        Worker Smashes Finger While Moving A Pallet O...\n",
      "23        Worker Injured In Fall Between Two Barges .  ...\n",
      "24        Employee Caught In Floors Suffers Lung Collap...\n",
      "25        Employee Falls Through Ceiling  Suffers Contu...\n",
      "26        Worker Thrown From Forklift Sustains Abrasion...\n",
      "27        Fall From Elevated Platform Injures Worker . ...\n",
      "28        Two Employees Are Struck By Log  One Sustains...\n",
      "29        Employee Suffers Lacerations When Struck By T...\n",
      "                               ...                        \n",
      "12800     Employee Sustains Hernia While Climbing Into ...\n",
      "12801     Employee'S Foot Caught In Nip Point  Later Am...\n",
      "12802     Employee Is Struck And Injured By Falling Hop...\n",
      "12803     Worker Installing Hydraulic Trench Shore Ampu...\n",
      "12804     Worker'S Thumb Is Amputated By Falling Partic...\n",
      "12805     Employee Is Injured When Struck By Cinder Blo...\n",
      "12806     Apprentice Iron Worker Is Injured While Erect...\n",
      "12807     Employee Is Struck By Falling Object And Rece...\n",
      "12808     Tie Down Binder Slips Strikes Worker And Frac...\n",
      "12809     Employee Is Injured When Load Dropped On His ...\n",
      "12810     Employee Is Killed When Struck By Loader .  O...\n",
      "12811     Employee Finger Smashed Between Planks .  At ...\n",
      "12812     Employee Injured When Struck By Falling Secti...\n",
      "12813     Employee'S Foot Injured During Steel Deck Wor...\n",
      "12814     Employee Suffers Contusions When Crane Overtu...\n",
      "12815     Employee Is Fatally Caught In Between The Bac...\n",
      "12816     Employee'S Fingers Are Injured When Caught Be...\n",
      "12817     Employee Is Burned By Hot Tar .  On November ...\n",
      "12818     Worker Is Crushed By Suspended Bag Of Sand . ...\n",
      "12819     Employee Is Killed When Buried Under Load Of ...\n",
      "12820     Employee Amputated His Finger On The Dumptruc...\n",
      "12821     Employee Is Injured While Moving Furniture . ...\n",
      "12822     Employee'S Finger Is Amputated By Truck Tailg...\n",
      "12823     Employee Is Injured By Falling Sheetrock .  A...\n",
      "12824     Employee Injured In Excavator Accident .  On ...\n",
      "12825     Employee Crushes Finger In Trailer Accident ....\n",
      "12826     Employee Killed When Struck By Falling Crane ...\n",
      "12827     Two Employees Injured When Struck By Load Fro...\n",
      "12828     Employee Is Killed While Unloading Glass Shee...\n",
      "12829     Employee Is Crushed Between Lift And Roof .  ...\n",
      "Name: Title_Summary_Case, Length: 12830, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Osha_AccidentCases['Title_Summary_Case'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_noun = myutilObj.tag_noun_func_words(Osha_AccidentCases['Title_Summary_Case'])\n",
    "text_noun = Osha_AccidentCases['Title_Summary_Case'].apply(myutilObj.tag_noun_func_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [Employee, Is, Forklift, Radiator, Fluid, Nove...\n",
      "1        [Employee, Falls, From, Flatbed, Trailer, Late...\n",
      "2        [Employee, Is, Struck, Bales, Wire, Killed, Au...\n",
      "3        [Employee, Hot, Water, Is, Burned, July, Emplo...\n",
      "4        [Employee, Suffers, Burns, Soup, June, Employe...\n",
      "5        [Employee, Self, Knife, incident, Employee, pi...\n",
      "6        [Foreman, Forklift, Tips, Over, May, Employee,...\n",
      "7        [Employee, Fractures, Abdomen, Run, Tug, Cart,...\n",
      "8        [Carpenter, Injured, Abdomen, Saw, Kicks, Back...\n",
      "9        [Employee, Abdomen, Head, Injury, Slip, Fall, ...\n",
      "10       [Employee, Sustains, Spleen, Board, Kicks, Bac...\n",
      "11       [Logger, Butt, Tree, Kicks, Out, Strikes, Him,...\n",
      "12       [Worker, Struck, Board, Thrown, Onto, Steps, I...\n",
      "13       [Employee, Suffers, Burns, Came, Contact, Live...\n",
      "14       [Bulldozer, Operator, Is, Crushed, Engine, Sep...\n",
      "15       [Employee, Abdomen, Is, Wood, Pallet, a.m., Se...\n",
      "16       [Employee, Fractures, Ankle, Unclogging, Beet,...\n",
      "17       [Employee, Is, Struck, Cable, Sustains, Facial...\n",
      "18       [Driver, Hurt, Truck, Unloading, Sand, Overtur...\n",
      "19       [Employee, Sprains, Knee, Struck, Forklift, Ju...\n",
      "20       [Employee, Falls, Through, Greenhouse, Roof, F...\n",
      "21       [Employee, Avalance, April, Employee, avalanch...\n",
      "22       [Worker, Smashes, Finger, A, Pallet, Cans, Apr...\n",
      "23       [Worker, Injured, Fall, Between, Barges, p.m.,...\n",
      "24       [Employee, Caught, Floors, Suffers, Lung, Coll...\n",
      "25       [Employee, Falls, Ceiling, Suffers, Contusions...\n",
      "26       [Worker, Thrown, From, Forklift, Sustains, Abr...\n",
      "27       [Fall, From, Elevated, Platform, Worker, Febru...\n",
      "28       [Employees, Log, One, Sustains, Fractures, Jan...\n",
      "29       [Employee, Suffers, Lacerations, Struck, Tree,...\n",
      "                               ...                        \n",
      "12800    [Employee, Sustains, Hernia, Into, Roller, Sep...\n",
      "12801    [Employee, Foot, Caught, Nip, Point, Later, Am...\n",
      "12802    [Employee, Is, Struck, Hopper, January, Employ...\n",
      "12803    [Worker, Installing, Hydraulic, Trench, Shore,...\n",
      "12804    [Worker, Thumb, Is, Particle, Boards, a.m., Ju...\n",
      "12805    [Employee, Struck, Cinder, Block, Later, Dies,...\n",
      "12806    [Apprentice, Iron, Worker, Is, Injured, Beam, ...\n",
      "12807    [Employee, Is, Struck, Object, Receives, Injur...\n",
      "12808    [Tie, Down, Binder, Slips, Strikes, Worker, Fr...\n",
      "12809    [Employee, Load, Foot, December, Employee, ton...\n",
      "12810    [Employee, Struck, Loader, November, Employee,...\n",
      "12811    [Employee, Finger, Smashed, Between, Planks, P...\n",
      "12812    [Employee, Struck, Section, Manhole, October, ...\n",
      "12813    [Employee, Foot, Injured, Steel, Deck, Work, A...\n",
      "12814    [Employee, Suffers, Contusions, Crane, Overtur...\n",
      "12815    [Employee, Between, Backhoe, Outrig, March, Em...\n",
      "12816    [Employee, Fingers, Caught, Between, Blocks, J...\n",
      "12817    [Employee, Is, Hot, Tar, November, Employee, p...\n",
      "12818    [Worker, Suspended, Bag, Sand, July, Employee,...\n",
      "12819    [Employee, Buried, Load, Dirt, a.m., March, em...\n",
      "12820    [Employee, Finger, Dumptruck, Tailgate, Door, ...\n",
      "12821    [Employee, Furniture, January, Employee, Brime...\n",
      "12822    [Employee, Finger, Is, Truck, Tailgate, Decemb...\n",
      "12823    [Employee, Sheetrock, a.m., July, Employee, fi...\n",
      "12824    [Employee, Injured, Excavator, Accident, June,...\n",
      "12825    [Employee, Crushes, Finger, Trailer, Accident,...\n",
      "12826    [Employee, Struck, Crane, Assembly, p.m., Marc...\n",
      "12827    [Employees, Struck, Load, From, Crane, cross, ...\n",
      "12828    [Employee, Glass, Sheets, pm, June, Employee, ...\n",
      "12829    [Employee, Between, Lift, Roof, November, Empl...\n",
      "Name: Title_Summary_Case, Length: 12830, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(text_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    # 1. Remove non-letters\n",
    "    sentence_text = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = sentence_text.lower().split()\n",
    "    # 3. Return a list of words\n",
    "    return(words)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def masia_to_sentences(masia, tokenizer, remove_stopwords=False ):\n",
    "    try:\n",
    "        # 1. Use the NLTK tokenizer to split the text into sentences\n",
    "        raw_sentences = tokenizer.tokenize(masia.strip())\n",
    "        # 2. Loop over each sentence\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            # If a sentence is empty, skip it\n",
    "            if len(raw_sentence) > 0:\n",
    "                # Otherwise, call sentence_to_wordlist to get a list of words\n",
    "                sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "        # 3. Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "        len(sentences)\n",
    "        return sentences\n",
    "    except:\n",
    "        print('nope')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nyt_masias = text_noun.tolist()\n",
    "\"\"\"nyt_masias = Osha_AccidentCases['Title_Summary_Case'].tolist()\n",
    "sentences = []\n",
    "\n",
    "for i in range(0,len(nyt_masias)):\n",
    "    try:\n",
    "        # Need to first change \"./.\" to \".\" so that sentences parse correctly\n",
    "        masia = nyt_masias[i].replace(\"/.\", '')\n",
    "        # Now apply functions\n",
    "        sentences += masia_to_sentences(masia, tokenizer)\n",
    "    except:\n",
    "        print('no!')\n",
    "\n",
    "print(\"There are \" + str(len(sentences)) + \" sentences in our corpus of osha.\")\n",
    "\n",
    "#Here is what one sentence list looks like:\n",
    "sentences[100]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-10-25 20:51:03,978 : INFO : collecting all words and their counts\n",
      "2017-10-25 20:51:03,979 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-25 20:51:04,113 : INFO : PROGRESS: at sentence #10000, processed 425185 words, keeping 25325 word types\n",
      "2017-10-25 20:51:04,142 : INFO : collected 28707 word types from a corpus of 542357 raw words and 12830 sentences\n",
      "2017-10-25 20:51:04,144 : INFO : Loading a fresh vocabulary\n",
      "2017-10-25 20:51:04,308 : INFO : min_count=1 retains 28707 unique words (100% of original 28707, drops 0)\n",
      "2017-10-25 20:51:04,309 : INFO : min_count=1 leaves 542357 word corpus (100% of original 542357, drops 0)\n",
      "2017-10-25 20:51:04,512 : INFO : deleting the raw counts dictionary of 28707 items\n",
      "2017-10-25 20:51:04,515 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-10-25 20:51:04,518 : INFO : downsampling leaves estimated 480386 word corpus (88.6% of prior 542357)\n",
      "2017-10-25 20:51:04,524 : INFO : estimated required memory for 28707 words and 300 dimensions: 83250300 bytes\n",
      "2017-10-25 20:51:04,709 : INFO : resetting layer weights\n",
      "2017-10-25 20:51:05,433 : INFO : training model with 4 workers on 28707 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-10-25 20:51:06,562 : INFO : PROGRESS: at 4.64% examples, 102312 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:07,633 : INFO : PROGRESS: at 9.01% examples, 100872 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:08,698 : INFO : PROGRESS: at 13.52% examples, 100354 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:09,792 : INFO : PROGRESS: at 17.99% examples, 99413 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:10,851 : INFO : PROGRESS: at 21.97% examples, 97892 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:12,048 : INFO : PROGRESS: at 25.20% examples, 92184 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:13,265 : INFO : PROGRESS: at 29.65% examples, 91420 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:14,604 : INFO : PROGRESS: at 34.10% examples, 89652 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:15,794 : INFO : PROGRESS: at 38.58% examples, 89571 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:16,805 : INFO : PROGRESS: at 42.54% examples, 90141 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:17,897 : INFO : PROGRESS: at 46.11% examples, 89321 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:18,937 : INFO : PROGRESS: at 49.87% examples, 88998 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:20,011 : INFO : PROGRESS: at 53.93% examples, 89099 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:21,026 : INFO : PROGRESS: at 57.32% examples, 88390 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:22,199 : INFO : PROGRESS: at 61.69% examples, 88519 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:23,314 : INFO : PROGRESS: at 65.96% examples, 88927 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:24,397 : INFO : PROGRESS: at 70.47% examples, 89444 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:25,400 : INFO : PROGRESS: at 74.16% examples, 89369 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:26,419 : INFO : PROGRESS: at 77.54% examples, 88813 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:27,424 : INFO : PROGRESS: at 80.79% examples, 88374 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:28,596 : INFO : PROGRESS: at 84.07% examples, 87327 words/s, in_qsize 8, out_qsize 0\n",
      "2017-10-25 20:51:29,676 : INFO : PROGRESS: at 87.29% examples, 86718 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:30,712 : INFO : PROGRESS: at 91.40% examples, 87011 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:31,798 : INFO : PROGRESS: at 94.74% examples, 86436 words/s, in_qsize 7, out_qsize 0\n",
      "2017-10-25 20:51:32,667 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-10-25 20:51:32,774 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-25 20:51:32,848 : INFO : PROGRESS: at 99.64% examples, 87323 words/s, in_qsize 1, out_qsize 1\n",
      "2017-10-25 20:51:32,850 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-25 20:51:32,878 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-25 20:51:32,880 : INFO : training on 2711785 raw words (2402101 effective words) took 27.4s, 87541 effective words/s\n",
      "2017-10-25 20:51:32,883 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-10-25 20:51:33,340 : INFO : saving Word2Vec object under osha_accidents, separately None\n",
      "2017-10-25 20:51:33,341 : INFO : not storing attribute syn0norm\n",
      "2017-10-25 20:51:33,344 : INFO : not storing attribute cum_table\n",
      "2017-10-25 20:51:35,940 : INFO : saved osha_accidents\n",
      "2017-10-25 20:51:35,942 : INFO : loading Word2Vec object from osha_accidents\n",
      "2017-10-25 20:51:36,577 : INFO : loading wv recursively from osha_accidents.wv.* with mmap=None\n",
      "2017-10-25 20:51:36,578 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-10-25 20:51:36,580 : INFO : setting ignored attribute cum_table to None\n",
      "2017-10-25 20:51:36,581 : INFO : loaded osha_accidents\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Implementation\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "num_features = 300    # Word vector dimensionality \n",
    "# Try and change min word count to see different results                     \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "model = word2vec.Word2Vec(text_noun, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg = 1) #skip gram sg =1\n",
    "\n",
    "#If you don’t plan to train the model any further, calling init_sims will make the model will be better for memory:\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "#Gensim makes it easy to store and load models:\n",
    "model_name = \"osha_accidents\"\n",
    "model.save(model_name)\n",
    "new_model = gensim.models.Word2Vec.load('osha_accidents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employee',\n",
       " 'Is',\n",
       " 'Forklift',\n",
       " 'Radiator',\n",
       " 'Fluid',\n",
       " 'November',\n",
       " 'Edco',\n",
       " 'Waste',\n",
       " 'Recycling',\n",
       " 'Services',\n",
       " 'forklift',\n",
       " 'Linde',\n",
       " 'Lift',\n",
       " 'Truck',\n",
       " 'Number',\n",
       " 'H2X393S04578',\n",
       " 'employer',\n",
       " 'FL-3',\n",
       " 'bales',\n",
       " 'paper',\n",
       " 'products',\n",
       " 'collection',\n",
       " 'area',\n",
       " 'yard',\n",
       " 'trucks']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Investigate the semantic space\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "vocab[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the word 'fell' exists in the vocabulary\n",
    "'plumber' in model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glove', 0.8870490789413452),\n",
       " ('tips', 0.8831332921981812),\n",
       " ('index', 0.8823497295379639),\n",
       " ('fingers', 0.87330162525177),\n",
       " ('spinning', 0.8721904158592224),\n",
       " ('flywheel', 0.8700660467147827),\n",
       " ('resistance', 0.8688862919807434),\n",
       " ('mechanisms', 0.867301344871521),\n",
       " ('cycling', 0.8667834401130676),\n",
       " ('amputations', 0.8647851943969727),\n",
       " ('spatula', 0.864740252494812),\n",
       " ('middle', 0.8634259104728699),\n",
       " ('nip', 0.8615032434463501),\n",
       " ('blades', 0.8612784743309021),\n",
       " ('knuckles', 0.8611034154891968),\n",
       " ('worm', 0.8608570098876953),\n",
       " ('groove', 0.8603649735450745),\n",
       " ('airlock', 0.8595898747444153),\n",
       " ('brushes', 0.859454870223999),\n",
       " ('sanding', 0.8594293594360352),\n",
       " ('platen', 0.8590328693389893),\n",
       " ('pinch', 0.8590306043624878),\n",
       " ('revolving', 0.8578372001647949),\n",
       " ('feeding', 0.8563242554664612),\n",
       " ('chopping', 0.855218768119812),\n",
       " ('catching', 0.8550158739089966),\n",
       " ('peddle', 0.8541698455810547),\n",
       " ('towel', 0.8540201783180237),\n",
       " ('knife', 0.853449821472168),\n",
       " ('shroud', 0.8532642126083374),\n",
       " ('lint', 0.853192925453186),\n",
       " ('pinky', 0.8528516888618469),\n",
       " ('fingertips', 0.8528465628623962),\n",
       " ('thumb', 0.8519221544265747),\n",
       " ('ring', 0.851853609085083),\n",
       " ('sliver', 0.8515620827674866),\n",
       " ('forefinger', 0.8512831926345825),\n",
       " ('micrometer', 0.8509318232536316),\n",
       " ('pointer', 0.8506012558937073),\n",
       " ('rag', 0.8505634069442749),\n",
       " ('backside', 0.850402295589447),\n",
       " ('turning', 0.8502057790756226),\n",
       " ('turret', 0.850070595741272),\n",
       " ('nip-point', 0.8483908176422119),\n",
       " ('mat', 0.8483375906944275),\n",
       " ('slipped', 0.847741961479187),\n",
       " ('digit', 0.8475567102432251),\n",
       " ('applicator', 0.8473777770996094),\n",
       " ('pusher', 0.8473697900772095),\n",
       " ('amputation', 0.8466699123382568),\n",
       " ('wrinkles', 0.8451606035232544),\n",
       " ('sponge', 0.844980001449585),\n",
       " ('cuttings', 0.8449629545211792),\n",
       " ('jaws', 0.8441954851150513),\n",
       " ('v-belt', 0.8439580202102661),\n",
       " ('running', 0.8437708616256714),\n",
       " ('stuffer', 0.8422239422798157),\n",
       " ('infeed', 0.8419564366340637),\n",
       " ('Lummus', 0.8413773775100708),\n",
       " ('jammed', 0.8412352204322815),\n",
       " ('screams', 0.8407472372055054),\n",
       " ('avulsion', 0.8407039642333984),\n",
       " ('pinkie', 0.8406896591186523),\n",
       " ('tip', 0.8403126001358032),\n",
       " ('outfeed', 0.8402166366577148),\n",
       " ('guillotine', 0.8385541439056396),\n",
       " ('inserts', 0.8380591869354248),\n",
       " ('feeder', 0.8376969695091248),\n",
       " ('suitcase', 0.8376495242118835),\n",
       " ('depositor', 0.8376472592353821),\n",
       " ('outermost', 0.8376069664955139),\n",
       " ('bowl', 0.8374230265617371),\n",
       " ('splinter', 0.83661949634552),\n",
       " ('flesh', 0.8364115357398987),\n",
       " ('clipper', 0.8361706733703613),\n",
       " ('mesh', 0.8361142873764038),\n",
       " ('ingoing', 0.8358975052833557),\n",
       " ('clamping', 0.8358834981918335),\n",
       " ('gears', 0.8350716829299927),\n",
       " ('planar', 0.8348464965820312),\n",
       " ('flap', 0.8348013162612915),\n",
       " ('thumbs', 0.8347071409225464),\n",
       " ('degloving', 0.8342860341072083),\n",
       " ('knives', 0.8338482975959778),\n",
       " ('left-hand', 0.8336131572723389),\n",
       " ('slot', 0.833564281463623),\n",
       " ('timing', 0.8333845734596252),\n",
       " ('sprockets', 0.8333579301834106),\n",
       " ('pull', 0.8330614566802979),\n",
       " ('one-third', 0.8327779173851013),\n",
       " ('trigger', 0.8325514197349548),\n",
       " ('inbound', 0.8324151039123535),\n",
       " ('wiper', 0.8323377370834351),\n",
       " ('paddle', 0.8322632312774658),\n",
       " ('recoiler', 0.8320112228393555),\n",
       " ('fingertip', 0.8316859006881714),\n",
       " ('impression', 0.8316845893859863),\n",
       " ('knuckle', 0.8316305875778198),\n",
       " ('sizer', 0.8309634923934937),\n",
       " ('rotor', 0.830891489982605)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.most_similar('hand',  topn=100)\n",
    "\n",
    "#The semantic vector for ‘fell’ looks like this\n",
    "#model['fell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Earlier we used unigrams, now using bigrams\n",
    "#bigramer = gensim.models.Phrases(sentences)\n",
    "#model = Word2Vec(bigramer[sentences], workers=num_workers, \\\n",
    "#            size=num_features, min_count = min_word_count, \\\n",
    "#            window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This can be done repeatedly to get tri-gram phrases (e.g. new_york_times)…but we’ll hold off. Here is the command incase you are interested\n",
    "#trigram = Phrases(bigram[sentence_stream])\n",
    "\n",
    "# semantic space looks like now that we’ve included bigrams\n",
    "#vocab = list(model.wv.vocab.keys())\n",
    "\n",
    "#vocab[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assessing the relationship of words in our semantic space. Which words are  most similar\n",
    "#model.most_similar('fell',  topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do these terms differ if we exclude a relationship between +ve and -ve\n",
    "#model.most_similar(positive=['start'], negative=['floor'], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word wiretap\n",
    "#model.most_similar('engine',  topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word\n",
    "#model.most_similar('worker',  topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What about the word ‘good’ without the word ‘bad’?\n",
    "#model.most_similar(positive=['turn'], negative=['pinned'], topn=15)\n",
    "\n",
    "#In case you want to add new sentences:\n",
    "#bigram.add_vocab(new_sentence_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
